# 2025-07-29 - 일일 기록

##  주요 업무 및 작업
- WDSR 논문 리뷰 완
- YOLOV5x,l,m 모델 학습 및 결과확인

##  논문 리뷰
배치정규화(BN)는 초해상도(SR)에서 문제가 많다 가중치 정규화(Weight Normalization)를 대안으로 제안
배치정규화가 SR에서 안 좋은 이유는 
- 통계 불안정: 작은 미니배치 → 평균/분산이 계속 바뀜 → 불안정한 정규화
- 불필요한 정규화: SR에서는 BN의 정규화 효과가 오히려 방해
- 훈련/테스트 차이: 공식이 달라서 결과가 다름 → 정확한 복원이 중요한 SR에서는 치명적

그럼에도 불구하고 깊은신경망학습을 하게되면 정규화가 필요 그러나 배치 정규화는 위와 같은 문제가있기에 핵심문제인 가중치를 정상화 시키는데 이때 가중치를 방향(v)과 크기(g)로 분리: w = (g/||v||)v 이렇게 투 트랙으로 학습하면 더 높은 학습률(10배) 과 정확도 개선 가능

네트워크 구조 개선
와이드 활성화: ReLU 전에 채널 확장
단순화: 5×5 단일 합성곱층으로 교체 → 더 적은 파라미터로 동일 성능

결론
ReLU이전에 최대한 특징을 확장시켜 확보하자 (WIDE)
정규화는 필요하지만 배치정규화 말고 가중치 정규화를 쓰자


##  모델 학습
X,L,M 모델을 학습진행 했고 지난번부터 신경쓰였던 mixup 파라마티는 0.0으로 변경하고 진행 굳이 이미지를 섞을 필요는 없을것같음

- X모델만 결과를 확인해보았는데 사람많을때는 성능 비슷한거같았고 누워있는 사람은 여전히 캐치못하고 메뉴판 인식 안하는줄 알았는데 마지막 부근에 마우스가 들어갔을때 캐치가 되었음 X모델은 MIXUP 설정을 0.2로 학습시킨거라 그것도 0.0으로 설정하고 내일 확인해봐야겠음


##  내일 할 일
- 모델 확인 및 평가
- LLM 강의자료 들어갈 컨텐츠 제작
- 보일러실 진행
